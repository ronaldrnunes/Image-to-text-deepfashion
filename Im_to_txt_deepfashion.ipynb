{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdqaXbuLiQ1a"
      },
      "source": [
        "# Problema Abordado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AhBEMWDUmAW"
      },
      "source": [
        "# Implementação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOmiCXs8T7mx"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7bNllyRyT_2Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision import models\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMtEMsuchLL"
      },
      "source": [
        "##Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aypah2WacsEP"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,          # Seu modelo\n",
        "    train_loader,   # DataLoader de treino\n",
        "    criterion,      # Função de perda\n",
        "    optimizer,      # Otimizador\n",
        "    device,         # \"cuda\" ou \"cpu\"\n",
        "    epochs=10       # Número de épocas\n",
        "):\n",
        "    model.to(device)\n",
        "    model.train()  # Modo treino\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for images, captions in train_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Loss médio da época\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkgUv_dR1TUd"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)), # Padrão dos modelos pretreinados do ImageNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406],  ## Normalizando dados no padrão do ImageNet\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YSmmlV8g1TUd"
      },
      "outputs": [],
      "source": [
        "#Função para ajustar dimensões dos tensores do dataset\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extrai cada campo do batch\n",
        "    images = [item[\"image_tensor\"] for item in batch]\n",
        "    captions = [item[\"caption\"] for item in batch]\n",
        "    input_ids = [item[\"tokenized_caption\"] for item in batch]\n",
        "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
        "    images_orig = [item[\"image_original\"] for item in batch]\n",
        "\n",
        "    # Empilha as imagens (todas já têm o mesmo shape, então stack direto)\n",
        "    image_tensor = torch.stack(images)\n",
        "\n",
        "    # Faz padding nas sequências de texto\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=50256)  # eos_token_id para GPT-2\n",
        "    attention_mask_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        \"image_tensor\": image_tensor,                    # (B, 3, 224, 224)\n",
        "        \"caption\": captions,                             # Lista de strings\n",
        "        \"tokenized_caption\": input_ids_padded,           # (B, T)\n",
        "        \"attention_mask\": attention_mask_padded,         # (B, T)\n",
        "        \"image_original\": images_orig                    # Lista de PIL Images\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39mL4ikCiW_a"
      },
      "source": [
        "## Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBiPU36gWSSU"
      },
      "source": [
        "###Carregando o dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LRCeV57L1TUe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DeepFashionDataset(Dataset):\n",
        "    def __init__(self, labelDataset, image_dir, transform=None, tokenizer=None):\n",
        "        self.df = labelDataset.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Tokenizador padrão: GPT-2\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # GPT-2 não tem pad_token original\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        caption = row[\"caption\"]\n",
        "        img_path = os.path.join(self.image_dir, row[\"path\"])\n",
        "\n",
        "        # Imagem original e transformada\n",
        "        image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "        image_tensor = self.transform(image_pil) if self.transform else image_pil\n",
        "\n",
        "        # Tokenização (sem truncamento, padding tratado no collate_fn no Dataloader)\n",
        "        tokens = self.tokenizer(caption, return_tensors=\"pt\")\n",
        "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = tokens[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"image_tensor\": image_tensor,\n",
        "            \"caption\": caption,\n",
        "            \"tokenized_caption\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"image_original\": image_pil\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA59r0s1VVUB",
        "outputId": "e73dc1fe-7449-4f5a-abae-d50281b1e3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12278 entries, 0 to 12277\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   image_id      12278 non-null  object\n",
            " 1   caption       12278 non-null  object\n",
            " 2   path          12278 non-null  object\n",
            " 3   gender        12278 non-null  object\n",
            " 4   product_type  12278 non-null  object\n",
            " 5   product_id    12278 non-null  object\n",
            " 6   image_type    12278 non-null  object\n",
            "dtypes: object(7)\n",
            "memory usage: 671.6+ KB\n"
          ]
        }
      ],
      "source": [
        "labels_df = pd.read_csv('datasets/labels_front.csv')\n",
        "labels_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qla7QbiWDzk"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3_hM9Ymb1TUf"
      },
      "outputs": [],
      "source": [
        "dataset = DeepFashionDataset(\n",
        "    labelDataset = labels_df,\n",
        "    image_dir = \"datasets/selected_images\",\n",
        "    transform = transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yJ0F13T61TUf"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 32 #Quantos exemplos são processsados juntos durante o treino\n",
        "#Como não estamos truncando o tamanho dos tokens precisamos garantir que todas as legendas\n",
        "# tenham o mesmo tamanho de tensor o collate_fn ajustara esse tamanho com\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnpjZSgZiWi2"
      },
      "source": [
        "## Rede Implementada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oAzQW_y3a2Vw"
      },
      "outputs": [],
      "source": [
        "class ResNetEncoder(nn.Module): #herda da classe nn.Module do pythorch\n",
        "    def __init__(self): ##inicializa a  classe resnetEncoder\n",
        "        super().__init__() ##inicializa a classe pai nn.Module\n",
        "\n",
        "        self.encoder = models.resnet50(pretrained=True) #Carrega resnet treinada com o imagenet. O pretreined=True garante que seja carregada com os pesos treinados com o imagenet\n",
        "        self.encoder.fc = nn.Identity() #Modifica a resnet para que a ultima camada não seja uma camada de classificação mas sim uma camada de identidade\n",
        "        #Não queremos classificar nada, apenas obter os pesos da rede para construir os \"embeddings de imagem\"\n",
        "\n",
        "    def forward(self, imgTensor):\n",
        "      return self.encoder(imgTensor) #Realiza foward na rede\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gpt2Decoder(nn.Module):\n",
        "  def __init__(self, image_dim=2048): #inicializa decoder\n",
        "    super().__init__() ##inicializa classe pai nn.module\n",
        "\n",
        "    self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    self.image_proj = nn.Linear(image_dim, self.gpt.config.n_embd) #ajustaa a dimensão de saida da resnet para a dimensão do embedding do gpt\n",
        "\n",
        "  def foward(self, textTokens, attention_mask, image_features):\n",
        "\n",
        "    batch_size = textTokens.size(0)\n",
        "\n",
        "    #Obtem embedding dos tokens e ajusta tamanho das imagens para produzir o input final\n",
        "    img_embed = self.image_proj(image_features).unsqueeze(1)\n",
        "    text_embeds = self.gpt.transformer.wte(textTokens)\n",
        "    gpt_input = torch.cat([img_embed, text_embeds], dim=1)\n",
        "\n",
        "    #Ajusta mascara de atenção pois agora temos a imagem concatenada\n",
        "    prefix_mask = torch.ones(batch_size, 1, dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "    full_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "    outputs = self.gpt(inputs_embeds=gpt_input, attention_mask=full_attention_mask)\n",
        "\n",
        "    return outputs.logits\n"
      ],
      "metadata": {
        "id": "I6j42OCh_-nJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptionModel(nn.Module):\n",
        ""
      ],
      "metadata": {
        "id": "ReWmH1fRoi_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9NpecFBiXTH"
      },
      "source": [
        "# Treinamento da Rede"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukwVKsITiXrC"
      },
      "source": [
        "# Qualidade dos Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUsn09Ei0V0"
      },
      "source": [
        "# Discussão Geral"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}