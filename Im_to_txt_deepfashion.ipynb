{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdqaXbuLiQ1a"
      },
      "source": [
        "# Problema Abordado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AhBEMWDUmAW"
      },
      "source": [
        "# Implementação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOmiCXs8T7mx"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7bNllyRyT_2Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision import models\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMtEMsuchLL"
      },
      "source": [
        "## Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aypah2WacsEP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        for batch in train_loader:\n",
        "            images = batch[\"image_tensor\"].to(device)\n",
        "            captions = batch[\"tokenized_caption\"].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            \n",
        "            # Calcula loss\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YSmmlV8g1TUd"
      },
      "outputs": [],
      "source": [
        "#Função para ajustar dimensões dos tensores do dataset\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extrai cada campo do batch\n",
        "    images = [item[\"image_tensor\"] for item in batch]\n",
        "    captions = [item[\"caption\"] for item in batch]\n",
        "    input_ids = [item[\"tokenized_caption\"] for item in batch]\n",
        "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
        "    images_orig = [item[\"image_original\"] for item in batch]\n",
        "\n",
        "    # Empilha as imagens (todas já têm o mesmo shape, então stack direto)\n",
        "    image_tensor = torch.stack(images)\n",
        "\n",
        "    # Faz padding nas sequências de texto\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=50256)  # eos_token_id para GPT-2\n",
        "    attention_mask_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        \"image_tensor\": image_tensor,                    # (B, 3, 224, 224)\n",
        "        \"caption\": captions,                             # Lista de strings\n",
        "        \"tokenized_caption\": input_ids_padded,           # (B, T)\n",
        "        \"attention_mask\": attention_mask_padded,         # (B, T)\n",
        "        \"image_original\": images_orig                    # Lista de PIL Images\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch[\"image_tensor\"].to(device)\n",
        "            captions = batch[\"tokenized_caption\"].to(device)\n",
        "            \n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39mL4ikCiW_a"
      },
      "source": [
        "## Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBiPU36gWSSU"
      },
      "source": [
        "### Carregando o dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LRCeV57L1TUe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DeepFashionDataset(Dataset):\n",
        "    def __init__(self, labelDataset, image_dir, transform=None, tokenizer=None):\n",
        "        self.df = labelDataset.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Tokenizador padrão: GPT-2\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # GPT-2 não tem pad_token original\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        caption = row[\"caption\"]\n",
        "        img_path = os.path.join(self.image_dir, row[\"path\"].replace(\"\\\\\", \"/\"))\n",
        "        img_path = os.path.normpath(img_path)\n",
        "        # Imagem original e transformada\n",
        "        image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "        image_tensor = self.transform(image_pil) if self.transform else image_pil\n",
        "\n",
        "        # Tokenização (sem truncamento, padding tratado no collate_fn no Dataloader)\n",
        "        tokens = self.tokenizer(caption, return_tensors=\"pt\")\n",
        "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = tokens[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"image_tensor\": image_tensor,\n",
        "            \"caption\": caption,\n",
        "            \"tokenized_caption\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"image_original\": image_pil\n",
        "        }\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)), # Padrão dos modelos pretreinados do ImageNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406],  ## Normalizando dados no padrão do ImageNet\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA59r0s1VVUB",
        "outputId": "e73dc1fe-7449-4f5a-abae-d50281b1e3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12278 entries, 0 to 12277\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   image_id      12278 non-null  object\n",
            " 1   caption       12278 non-null  object\n",
            " 2   path          12278 non-null  object\n",
            " 3   gender        12278 non-null  object\n",
            " 4   product_type  12278 non-null  object\n",
            " 5   product_id    12278 non-null  object\n",
            " 6   image_type    12278 non-null  object\n",
            "dtypes: object(7)\n",
            "memory usage: 671.6+ KB\n"
          ]
        }
      ],
      "source": [
        "labels_df = pd.read_csv('datasets/labels_front.csv')\n",
        "labels_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qla7QbiWDzk"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3_hM9Ymb1TUf"
      },
      "outputs": [],
      "source": [
        "dataset = DeepFashionDataset(\n",
        "    labelDataset = labels_df,\n",
        "    image_dir = \"datasets/selected_images\",\n",
        "    transform = transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yJ0F13T61TUf"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "batch_size = 32 #Quantos exemplos são processsados juntos durante o treino\n",
        "#Como não estamos truncando o tamanho dos tokens precisamos garantir que todas as legendas\n",
        "# tenham o mesmo tamanho de tensor o collate_fn ajustara esse tamanho com\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnpjZSgZiWi2"
      },
      "source": [
        "## Rede Implementada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oAzQW_y3a2Vw"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, output_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Camadas convolucionais\n",
        "        self.features = nn.Sequential(\n",
        "            # Bloco 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Bloco 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Bloco 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Bloco 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        \n",
        "        # Projeção para a dimensão desejada\n",
        "        self.projection = nn.Linear(256, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I6j42OCh_-nJ"
      },
      "outputs": [],
      "source": [
        "class Gpt2Decoder(nn.Module):\n",
        "    def __init__(self, image_dim=256): \n",
        "        super().__init__()\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        \n",
        "       \n",
        "        for param in self.gpt.parameters():\n",
        "            param.requires_grad = False\n",
        "       \n",
        "        for param in self.gpt.transformer.h[-4:].parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.gpt.lm_head.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        self.image_proj = nn.Linear(image_dim, self.gpt.config.n_embd)\n",
        "        \n",
        "    def forward(self, textTokens, image_features):\n",
        "        batch_size = textTokens.size(0)\n",
        "        \n",
        "        \n",
        "        img_embed = self.image_proj(image_features).unsqueeze(1)\n",
        "        \n",
        "        \n",
        "        text_embeds = self.gpt.transformer.wte(textTokens)\n",
        "        \n",
        "        gpt_input = torch.cat([img_embed, text_embeds], dim=1)\n",
        "        \n",
        "        attention_mask = (textTokens != 50256).float()  \n",
        "        prefix_mask = torch.ones(batch_size, 1, dtype=attention_mask.dtype, \n",
        "                               device=attention_mask.device)\n",
        "        full_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "        \n",
        "        outputs = self.gpt(inputs_embeds=gpt_input, attention_mask=full_attention_mask)\n",
        "        return outputs.logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ReWmH1fRoi_D"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, encoder_output_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(output_dim=encoder_output_dim)\n",
        "        \n",
        "        self.decoder = Gpt2Decoder(image_dim=encoder_output_dim)\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        image_features = self.encoder(images)\n",
        "        logits = self.decoder(captions, image_features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImageCaptionModel().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=50256)  # Ignora padding tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9NpecFBiXTH"
      },
      "source": [
        "# Treinamento da Rede"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets\\\\selected_images\\\\WOMEN-Tees_Tanks-id_00001690-02_1_front.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m         images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         captions \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "Cell \u001b[1;32mIn[30], line 20\u001b[0m, in \u001b[0;36mDeepFashionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(img_path)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Imagem original e transformada\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image_pil) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;28;01melse\u001b[39;00m image_pil\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Tokenização (sem truncamento, padding tratado no collate_fn no Dataloader)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets\\\\selected_images\\\\WOMEN-Tees_Tanks-id_00001690-02_1_front.jpg'"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, criterion, optimizer, device, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukwVKsITiXrC"
      },
      "source": [
        "# Qualidade dos Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUsn09Ei0V0"
      },
      "source": [
        "# Discussão Geral"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
